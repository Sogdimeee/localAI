# core/bot.py
from transformers import pipeline

from transformers import pipeline


class QuestionAnsweringBot2:
    _instance = None
    _generator = None

    def __new__(cls, model_name="gpt2"):
        if cls._instance is None:
            print("ðŸ”„ Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ÑÑ GPT-2...")
            cls._instance = super(QuestionAnsweringBot2, cls).__new__(cls)
            cls._generator = pipeline("text-generation", model=model_name)
            print("âœ… GPT-2 Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½.")
        return cls._instance

    def ask(self, question: str) -> str:
        # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚
        prompt = f"Ð’Ð¾Ð¿Ñ€Ð¾Ñ: {question}\nÐžÑ‚Ð²ÐµÑ‚:"

        result = self._generator(
            prompt,
            max_length=150,  # ÑƒÐ²ÐµÐ»Ð¸Ñ‡ÑŒ Ð´Ð¾ 200â€“300 Ð¿Ñ€Ð¸ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸
            num_return_sequences=1,
            do_sample=True,  # Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€ÑÐ» Ð¾Ð´Ð½Ð¾ Ð¸ Ñ‚Ð¾ Ð¶Ðµ
            top_p=0.95,
            temperature=0.8  # Ñ€ÐµÐ³ÑƒÐ»Ð¸Ñ€ÑƒÐµÑ‚ ÐºÑ€ÐµÐ°Ñ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ
        )

        return result[0]['generated_text'][len(prompt):].strip()


class QuestionAnsweringBot:
    _instance = None
    _qa_pipeline = None

    def __new__(cls, model_name="deepset/roberta-base-squad2"):
        if cls._instance is None:
            print("ðŸ”„ Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÑŒ...")
            cls._instance = super(QuestionAnsweringBot, cls).__new__(cls)
            cls._qa_pipeline = pipeline("question-answering", model=model_name)
            print("âœ… ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð°.")
        return cls._instance

    def ask(self, question: str, context: str) -> str:
        result = self._qa_pipeline(question=question, context=context)
        return result['answer']
